{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN79ulVSQpQ5K2JpSHEb5YH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christpaul94/MastersThesis_PaulChrist/blob/main/GPU_MatrixMultiplicationDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyZDEy0M_D6D",
        "outputId": "6ddc9d33-5036-4a96-f00e-eac634564b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n",
            "Benchmark MatMul: M=4096, N=4096, K=4096, dtype=torch.float32\n",
            "Wiederholungen: 1\n",
            "\n",
            "Korrekte Berechnung PyTorch\n",
            "Korrekte Berechnung Triton\n",
            "\n",
            "--- Benchmark-Ergebnisse (MatMul) ---\n",
            "NumPy (CPU):             1752.0670 ms\n",
            "PyTorch (GPU, cuBLAS):   47.8194 ms\n",
            "Manueller Triton (GPU):  38.6560 ms\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# ========================================================================\n",
        "# 1. TRITON MATMUL KERNEL\n",
        "# ========================================================================\n",
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "    ],\n",
        "    key=['M', 'N', 'K'],\n",
        ")\n",
        "# Eigener kernel\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    A_ptr, B_ptr, C_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "    BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE_M: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(axis=0)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    pid_m = pid // num_pid_n\n",
        "    pid_n = pid % num_pid_n\n",
        "    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M))\n",
        "    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N))\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "    for k in range(0, K, BLOCK_SIZE_K):\n",
        "        offs_k = k + tl.arange(0, BLOCK_SIZE_K)\n",
        "        a_ptrs = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "        a_tile = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n",
        "        b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n",
        "        b_tile = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)\n",
        "        accumulator += tl.dot(a_tile, b_tile)\n",
        "    c_ptrs = C_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n",
        "    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n",
        "\n",
        "\n",
        "def matmul(a, b):\n",
        "    M, K = a.shape\n",
        "    K, N = b.shape\n",
        "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
        "    grid = lambda META: (\n",
        "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
        "    )\n",
        "    matmul_kernel[grid](\n",
        "        a, b, c,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "    )\n",
        "    return c\n",
        "\n",
        "# ========================================================================\n",
        "# 2. BENCHMARK\n",
        "# ========================================================================\n",
        "\n",
        "# --- Parameter ---\n",
        "M = N = K = 2**12\n",
        "print(M)\n",
        "dtype = torch.float32\n",
        "device_gpu = 'cuda'\n",
        "device_cpu = 'cpu'\n",
        "\n",
        "N_REPEATS_GPU = N_REPEATS_CPU =  1\n",
        "\n",
        "print(f\"Benchmark MatMul: M={M}, N={N}, K={K}, dtype={dtype}\")\n",
        "print(f\"Wiederholungen: {N_REPEATS_CPU}\\n\")\n",
        "\n",
        "# --- Daten erstellen ---\n",
        "a_gpu = torch.randn(M, K, device=device_gpu, dtype=dtype).contiguous()\n",
        "b_gpu = torch.randn(K, N, device=device_gpu, dtype=dtype).contiguous()\n",
        "a_cpu = a_gpu.to(device_cpu)\n",
        "b_cpu = b_gpu.to(device_cpu)\n",
        "\n",
        "a_np = a_cpu.numpy()\n",
        "b_np = b_cpu.numpy()\n",
        "\n",
        "# print(a_np)\n",
        "# print(b_np)\n",
        "\n",
        "# --- CUDA Events f체r pr채zises GPU-Timing ---\n",
        "start_event = torch.cuda.Event(enable_timing=True)\n",
        "end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "\n",
        "# --- 2. Benchmark: NumPy (CPU) ---\n",
        "c_np = np.zeros((M, N), dtype=a_np.dtype)\n",
        "\n",
        "# Messung\n",
        "start_time_np = time.perf_counter()\n",
        "for _ in range(N_REPEATS_CPU):\n",
        "    c_np = np.dot(a_np, b_np) # np.dot ist der Standard f체r MatMul\n",
        "end_time_np = time.perf_counter()\n",
        "np_time_ms = ((end_time_np - start_time_np) * 1000) / N_REPEATS_CPU\n",
        "#print(c_np)\n",
        "\n",
        "# --- 3. Benchmark: torch.matmul (GPU, cuBLAS Referenz) ---\n",
        "c_torch = torch.empty(M, N, device=device_gpu, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "# Messung\n",
        "start_event.record()\n",
        "for _ in range(N_REPEATS_GPU):\n",
        "    c_torch = torch.matmul(a_gpu, b_gpu)\n",
        "end_event.record()\n",
        "torch.cuda.synchronize()\n",
        "torch_time_ms = start_event.elapsed_time(end_event) / N_REPEATS_GPU\n",
        "\n",
        "\n",
        "# --- 4. Benchmark: Manueller Triton-Kernel (GPU) ---\n",
        "c_triton = torch.empty(M, N, device=device_gpu, dtype=dtype)\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    c_triton = matmul(a_gpu, b_gpu)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Messung\n",
        "start_event.record()\n",
        "for _ in range(N_REPEATS_GPU):\n",
        "    c_triton = matmul(a_gpu, b_gpu)\n",
        "end_event.record()\n",
        "torch.cuda.synchronize()\n",
        "triton_time_ms = start_event.elapsed_time(end_event) / N_REPEATS_GPU\n",
        "\n",
        "\n",
        "# --- 5. Korrektheitspr체fung ---\n",
        "try:\n",
        "    # Vergleiche Torch-CPU und NumPy\n",
        "    np.allclose(c_np, c_torch.cpu().numpy(), atol=1e-5, rtol=1e-4)\n",
        "    print(\"Korrekte Berechnung PyTorch\")\n",
        "\n",
        "    # Vergleiche Triton mit der Torch-GPU-Referenz\n",
        "    np.allclose(c_np, c_triton.cpu().numpy(), atol=1e-5, rtol=1e-4)\n",
        "    print(\"Korrekte Berechnung Triton\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  Inkorrekte Berechnung. Fehler: {e}\")\n",
        "\n",
        "# --- 6. Ergebnisse ---\n",
        "print(\"\\n--- Benchmark-Ergebnisse (MatMul) ---\")\n",
        "\n",
        "print(f\"NumPy (CPU):             {np_time_ms:.4f} ms\")\n",
        "print(f\"PyTorch (GPU, cuBLAS):   {torch_time_ms:.4f} ms\")\n",
        "print(f\"Manueller Triton (GPU):  {triton_time_ms:.4f} ms\")\n"
      ]
    }
  ]
}